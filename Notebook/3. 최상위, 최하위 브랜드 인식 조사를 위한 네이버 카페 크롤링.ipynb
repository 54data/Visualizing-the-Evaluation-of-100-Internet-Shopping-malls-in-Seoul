{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19971941",
   "metadata": {},
   "source": [
    "### 3. 최상위, 최하위 브랜드 인식 조사를 위한 네이버 카페 크롤링  \n",
    "\n",
    "- 〈서울시 인터넷 쇼핑몰 100개 평가 정보〉에 따르면 2020년도 전체평가 기준 최고 점수를 받은 브랜드는 '마켓컬리'이며 최하위점수를 받은 브랜드는 '트립닷컴'이다.  \n",
    "- '마켓컬리'와 '트립닷컴'에 대한 내용을 담은 네이버 카페 글(제목, 내용, 댓글)을 크롤링하여 텍스트 파일로 생성했다.  \n",
    "- 네이버 카페 검색 기준 : 일반글, 작성일자(2020/01/01-2020/12/31), 관련도 순으로 정렬  \n",
    "- 함수로 생성하여 검색 키워드와 몇 개의 글을 크롤링 할 것인지를 입력하도록 하였다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abe733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naver_cafe(keyword, num):\n",
    "\n",
    "    # 모듈\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "    from tqdm.notebook import tqdm\n",
    "    import time\n",
    "    import re\n",
    "\n",
    "    # 크롬 드라이버\n",
    "    s = Service(\"c:/data/chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service = s)\n",
    "\n",
    "    driver.get(\"https://search.naver.com/search.naver?where=article&query=\" + keyword)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 옵션 클릭\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[1]/div/div[2]\").click()\n",
    "        \n",
    "    # 일반글 선택\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[2]/ul/li[2]/div/div/a[3]\").click()\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # 관련도 순으로 정렬\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[2]/ul/li[3]/div/div/a[1]\").click()\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # 기간 직접 입력\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[2]/ul/li[4]/div/div[1]/a[9]\").click()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 기간 설정 2020/01/01 - 2020/12/31\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[2]/ul/li[4]/div/div[2]/div[2]/div[1]/div/div/div/ul/li[31]\").click()\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[2]/ul/li[4]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[1]\").click()\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[2]/ul/li[4]/div/div[2]/div[1]/span[3]/a\").click()\n",
    "        \n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[2]/ul/li[4]/div/div[2]/div[2]/div[1]/div/div/div/ul/li[31]\").click()\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[2]/ul/li[4]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[12]\").click()\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"snb\\\"]/div[2]/ul/li[4]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[31]\").click()\n",
    "    \n",
    "    # 기간 적용\n",
    "    driver.find_element(By.CLASS_NAME, \"btn_apply._apply_btn\").click()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 스크롤 끝까지 내리기\n",
    "    last_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "    params = []\n",
    "    \n",
    "    while True:\n",
    "        # 스크롤을 아래로 내리기\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 지금까지 스크롤 내린 현재 페이지의 높이를 가져오기\n",
    "        new_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        \n",
    "        # 현재 페이지 높이가 이 페이지의 마지막이면 끝내기\n",
    "        if new_page_height == last_page_height:\n",
    "    \n",
    "            html = driver.page_source\n",
    "            time.sleep(2)\n",
    "            \n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            txt = soup.select(\"div.total_area > a\")\n",
    "\n",
    "            for i in txt:\n",
    "                params.append(i.get('href'))\n",
    "                \n",
    "            break\n",
    "\n",
    "        # 그렇지 않으면 마지막 높이가 될 때까지 진행하는 것\n",
    "        last_page_height = new_page_height\n",
    "    \n",
    "    # 중복이 있는지 확인(순서 유지)\n",
    "    params2 = list(dict.fromkeys(params))\n",
    "        \n",
    "    # 번호 세기\n",
    "    cnt = 1\n",
    "    \n",
    "    # 텍스트 파일 생성\n",
    "    f = open(\"c:/data/cafe_data/\"+ keyword + \".txt\", \"w\", encoding = \"utf8\")\n",
    "    \n",
    "    for i in tqdm(params2):\n",
    "        driver.get(i)\n",
    "        time.sleep(7)\n",
    "        \n",
    "        try: \n",
    "            driver.switch_to.frame(\"cafe_main\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            html2 = driver.page_source\n",
    "            soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "                       \n",
    "            f.write('[' + str(cnt) + ']')\n",
    "            \n",
    "            # 제목 크롤링\n",
    "            title = soup2.select('div.title_area > h3')\n",
    "            for i in title:\n",
    "                f.write(re.sub('[\\n\\r\\t]','',i.text).lstrip(' ') + '\\n')\n",
    "            \n",
    "            # 내용 크롤링\n",
    "            content = soup2.select('div.se-main-container')\n",
    "            if len(content) > 0 :\n",
    "                for i in content:\n",
    "                    f.write(re.sub('[\\n\\r\\t]','',i.text) + '\\n')\n",
    "            \n",
    "            else:\n",
    "                content2 = soup2.select('div.ContentRenderer')\n",
    "                for i in content2:\n",
    "                    f.write(re.sub('[\\n\\r\\t]','',i.text) + '\\n')  \n",
    "            \n",
    "            time.sleep(2)\n",
    "            \n",
    "            # 댓글 크롤링\n",
    "            comment = soup2.select('span.text_comment')\n",
    "            com_t = '' \n",
    "            for i in comment:\n",
    "                if keyword in i.text:\n",
    "                    com_t += re.sub('[\\n\\r\\t]','',i.text) + ' '\n",
    "                            \n",
    "            time.sleep(1)\n",
    "            \n",
    "            f.write(com_t + '\\n\\n')\n",
    "            \n",
    "            # 게시글을 num만큼 가져오기\n",
    "            cnt += 1\n",
    "            if cnt > num:\n",
    "                break\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_cafe('마켓컬리', 200)\n",
    "naver_cafe('트립닷컴', 200)\n",
    "# 200개의 글을 크롤링하여 텍스트 파일로 생성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
